---
title: "response to reviewer round 2"
author: "Stijn Masschelein"
date: "11/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Variation of yhat - Distribution of maximum

You can treat $y_{max}$ as a an order statistic. If 
$F(y) \sim \text{U}[0, 1)$ is the cumulative distribution of profit,
then the cumulative density function $y_max$ is 
$F(y_{max} \sim \text{beta}(O, 1)$

```{r}
require(viridis)
O <- c(1, 2, 4, 8, 16, 32, 64)
colors <- viridis(length(O) + 1)
N <- 1e3
performance <- seq(-4, 4, 12/N)
cumulative <- pnorm(performance)
distribution <- dnorm(performance)

par(mfrow = c(1, 3))
plot(x = performance, y = cumulative, 
     xlab = "performance", ylab = "cumulative quantiles",
     type = "l", col = colors[1])
for(o in 1:length(O)){
  lines(x = performance, 
        y = pbeta(cumulative, o, 1),
        col = colors[o + 1])
}
plot(x = performance, y = distribution, 
     xlab = "performance", ylab = "frequency",
     type = "l", col = colors[1], ylim = c(0, .7))
for(o in 1:length(O)){
  lines(x = performance, 
        y = o * cumulative ^(o - 1) * distribution,
        col = colors[o + 1])
}
plot(x = performance, y = distribution, 
     xlab = "performance", ylab = "frequency",
     type = "l", col = colors[1], ylim = c(0, .7))
for(o in 1:length(O)){
  lines(x = performance, 
        y = distribution * dbeta(cumulative, o, 1),
        col = colors[o + 1])
}
```

The idea is that we have the pdf $f(y)$ and the cumulative 
distribution $F(y)$ of the underlying profits.


$$
\begin{aligned}
F(y_{(n)}) &= F(y)^n \\
f(y_{(n)}) &= n{F(y)}^{n - 1} f(y) \\
\end{aligned}
$$

And for the Beta inversion method with $U = Uniform(0,1)$. 

$$
\begin{aligned}
U_{(n)} &\sim Beta(n, 1) \\
u &= F_{X}(x) \\
du &= f_{X}(x) dx  \\
F_X(x_{(n)}) &\sim Beta(n, 1) \\
X_{(n)} &\sim Beta(n, 1) \frac{du}{dx} && \text{Jacobian for change of var?} \\
&\sim Beta(n, 1) \times f_X(x) && \text{Change of variable du}
\end{aligned}
$$

### Test of beta approach

```{r test-beta-adj}
par(mfrow = c(1, 1))
Otest <- 2
sample <- replicate(1e5, max(rnorm(Otest)))
plot(density(sample))
lines(x = performance, y = distribution * dbeta(cumulative, Otest, 1))
```

## Sum of two variables.

Say $Y = .5 * X^1 + .5 * X^2$. 

$$
\begin{aligned}
\frac{dy}{dx^i} &= .5 \\
f(y_{(n)}) &= n F(y) ^ {n - 1} f(y) \\
f(x^i_{y(n)}) &= f(y_{(n)}) \frac{dy}{dx^i} \\
\end{aligned}
$$

### Test of exponential approach

```{r exponential_sum_of_2}
Otest <- 16
w <- sqrt(.5)
sampling = function(Otest = 1, w = sqrt(.5)){
  x1 <- rnorm(Otest) 
  x2 <- rnorm(Otest)
  y <- w * x1 + w * x2; ymax = max(y)
  return(c(yo = ymax, x1o = x1[y == ymax], x2o = x2[y == ymax]))
}
sample = t(replicate(1e5, sampling(Otest = Otest)))
dat_sample = as.data.frame(sample)
sum(abs(dat_sample$yo - w * (dat_sample$x1o + dat_sample$x2o)))
par(mfrow = c(1, 1))
plot(density(dat_sample$yo))
lines(x = performance, y = distribution * dbeta(cumulative, Otest, 1))
```

```{r sum_of_2_X}
library(lattice)
library(latticeExtra)
dens_x = function(O = 1, x1 = 0, x2 = 0){
  X <- expand.grid(x1, x2)
  w <- sqrt(.5)
  d <- O * pnorm(w * X[, 1] + w * X[, 2]) ^ (O - 1) * 
    dnorm(X[, 1]) * dnorm(X[, 2])
  result <- cbind(X, d)
  names(result)<- c("x1", "x2", "d")
  return(result)
}
N <- 1e2
x1 <- seq(-3, 3, 6/N)
x2 <- seq(-3, 3, 6/N)
dat <- as.data.frame(dens_x(O = Otest, x1 = x1, x2 = x2))
p1 <- contourplot(d ~ x1 * x2, data = dat, 
            cuts = 10, pretty = TRUE)
p2 <- xyplot(dat_sample$x1 ~ dat_sample$x2, 
             alpha = .02, xlab = "x2", ylab = "x1")
print(p2 + as.layer(p1))
```

This means [^collider] that the joint distribution can be written as
$f(x_1, x_2) = O F_Y(\sqrt{2} (x1 + x2))^{O - 1} f_x(x_1) f_x(x_2)$

[^collider]: It also looks like that this is collider bias. Does that open the possibility for bias in the demand specification?]

## Sum of two discrete

If $y = \beta x_1 + \beta x_2 + \epsilon$ 
with $x_1$ and $x_2$ binomial (-1, 1) then
we can think of $y_(n)$ as a mixture of four normals. 

The joint distribution becomes. 

$$
O(\Phi(2 \beta/\sigma_{\nu}) + 2 \Phi(0) + \Phi(- 2 \beta / \sigma_{\nu}))^{O - 1}  Binomial(x_1)Binomial(x_2)\color{red}{N(0, \sigma_{\nu})}
$$

Integrating out $x_1$ and $x_2$ is easy it's just 1/2 for each option.

Probability of making a mistake between two adjacent options $\Phi(-\beta/ \sqrt(4) \sigma_\{nu})$. 

Probability of $2 \beta > 0$ equals $\Phi(2 * \beta_1/ \sqrt{2} (\gamma_1^2 + \sigma^2_{\epsilon_1}))$
